# AI 大模型微调训练营第3期 -- 毕业总结
### 姓名：钟超 

5月14日，刚刚结束了第4期的大模型应用开发实战营学习，我就趁热继续投入了为期9周的大模型微调训练营学习。因为有了前面的互相了解，我非常钦佩彭靖田老师的技术实力和教学水平，也很信赖极客时间出品的课程质量，所以报名后也没有什么多余的担心。

在新的训练营中，我熟练掌握了：
1. 运用云服务器来搭建自己的大模型微调训练环境，通过vscode + ssh 在本地开发环境进行远程开发和调试。
2. 用deepspeed分布式框架来加速大模型的训练，即便单机单卡，也能通过zero-3相关技术来充分利用CPU、内存和NVMe的硬件资源，以时间换空间的策略来完成训练任务。
3. 用Huggingface的transformers库来构建大模型微调训练任务，用datasets库来加载和构建数据集，用evaluate库来评估模型效果。
4. 用peft微调，指令微调和参数量化技术对chatglm3-6b, Llama-2-7b, facebook/opt, openai/whisper等多个知名开源大模型进行微调，完成广告语生成，语音识别，指令跟随等下游任务。

两个多月的学习下来，我不仅学到了很多前沿的大模型微调技术，还结识了很多志同道合的朋友，一起进步，一起成长。我也越来越有信心去寻找新的机遇，迎接新的挑战。

感谢彭靖田老师，班班，助教，感谢极客时间和智谱AI，还有一起学习的朋友们，感谢大家。


