{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 智谱章节作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 使用官方提供的示例，成功微调出广告数据集，要求使用 Lora 进行微调：  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 你能看到 loss 的下降，并在最终回到 3.2 左右。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [\n",
    "                        {'role': 'user', 'content': dct['content']},\n",
    "                        {'role': 'assistant', 'content': dct['summary']}\n",
    "                        ]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "    \n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:02<00:00,  3.15it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.0312\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "You are adding a <class 'transformers.integrations.integration_utils.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "resume checkpoint from  checkpoint-2200\n",
      "Loading model from ./output/checkpoint-2200.\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 2200\n",
      "  Will skip the first 0 epochs then the first 2200 batches in the first epoch.\n",
      "{'loss': 3.4537, 'grad_norm': 6.896091461181641, 'learning_rate': 1.3166666666666665e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4189, 'grad_norm': 7.814781188964844, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4186, 'grad_norm': 8.302922248840332, 'learning_rate': 1.2833333333333333e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3766, 'grad_norm': 8.30824089050293, 'learning_rate': 1.2666666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2404, 'grad_norm': 8.464018821716309, 'learning_rate': 1.25e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3664, 'grad_norm': 7.929145336151123, 'learning_rate': 1.2333333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4287, 'grad_norm': 8.833356857299805, 'learning_rate': 1.2166666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4689, 'grad_norm': 7.764798641204834, 'learning_rate': 1.2e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2936, 'grad_norm': 8.392908096313477, 'learning_rate': 1.1833333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3602, 'grad_norm': 8.469298362731934, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.08}\n",
      " 77%|█████████████████████████████▉         | 2300/3000 [01:00<01:08, 10.22it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      " 77%|█████████████████████████████▉         | 2300/3000 [01:13<01:08, 10.22it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:14<00:14,  7.36s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:16<00:05,  5.19s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:31<00:00,  8.68s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.841 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 32.049656000000006, 'eval_rouge-2': 7.1878340000000005, 'eval_rouge-l': 24.031231999999996, 'eval_bleu-4': 0.03339858697518586, 'eval_runtime': 47.9522, 'eval_samples_per_second': 1.043, 'eval_steps_per_second': 0.083, 'epoch': 0.08}\n",
      " 77%|█████████████████████████████▉         | 2300/3000 [01:48<01:08, 10.22it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:32<00:00,  8.68s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2300\n",
      "/root/miniconda3/envs/deepspeed/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /root/huggingface/hub/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Deleting older checkpoint [output/checkpoint-1800] due to args.save_total_limit\n",
      "{'loss': 3.3188, 'grad_norm': 8.531181335449219, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3291, 'grad_norm': 8.559377670288086, 'learning_rate': 1.1333333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3682, 'grad_norm': 9.235170364379883, 'learning_rate': 1.1166666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3662, 'grad_norm': 7.94266414642334, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2742, 'grad_norm': 8.777070999145508, 'learning_rate': 1.0833333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3846, 'grad_norm': 8.412117958068848, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3496, 'grad_norm': 7.931662559509277, 'learning_rate': 1.05e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4963, 'grad_norm': 8.637223243713379, 'learning_rate': 1.0333333333333333e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2367, 'grad_norm': 8.515596389770508, 'learning_rate': 1.0166666666666667e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4561, 'grad_norm': 7.7916388511657715, 'learning_rate': 1e-05, 'epoch': 0.08}\n",
      " 80%|███████████████████████████████▏       | 2400/3000 [02:47<06:20,  1.58it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:16<00:16,  8.26s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:19<00:05,  5.88s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.043124000000006, 'eval_rouge-2': 7.067508, 'eval_rouge-l': 24.986397999999998, 'eval_bleu-4': 0.032559058257843804, 'eval_runtime': 38.5943, 'eval_samples_per_second': 1.296, 'eval_steps_per_second': 0.104, 'epoch': 0.08}\n",
      " 80%|███████████████████████████████▏       | 2400/3000 [03:26<06:20,  1.58it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:21<00:00,  4.28s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2400\n",
      "/root/miniconda3/envs/deepspeed/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /root/huggingface/hub/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Deleting older checkpoint [output/checkpoint-1900] due to args.save_total_limit\n",
      "{'loss': 3.4584, 'grad_norm': 8.163753509521484, 'learning_rate': 9.833333333333333e-06, 'epoch': 0.08}\n",
      "{'loss': 3.2773, 'grad_norm': 8.131178855895996, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.08}\n",
      "{'loss': 3.3719, 'grad_norm': 7.591523170471191, 'learning_rate': 9.5e-06, 'epoch': 0.08}\n",
      "{'loss': 3.3795, 'grad_norm': 8.018631935119629, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2771, 'grad_norm': 7.863231658935547, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3148, 'grad_norm': 7.855373382568359, 'learning_rate': 9e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2576, 'grad_norm': 8.887500762939453, 'learning_rate': 8.833333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4367, 'grad_norm': 7.622372150421143, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.09}\n",
      "{'loss': 3.474, 'grad_norm': 8.122747421264648, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3902, 'grad_norm': 9.815075874328613, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.09}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [04:25<04:46,  1.74it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:15<00:15,  7.60s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:30<00:10, 10.71s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.212592000000004, 'eval_rouge-2': 7.127184, 'eval_rouge-l': 23.131026, 'eval_bleu-4': 0.031630882052273004, 'eval_runtime': 61.9827, 'eval_samples_per_second': 0.807, 'eval_steps_per_second': 0.065, 'epoch': 0.09}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [05:27<04:46,  1.74it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:45<00:00, 12.41s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2500\n",
      "/root/miniconda3/envs/deepspeed/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /root/huggingface/hub/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Deleting older checkpoint [output/checkpoint-2000] due to args.save_total_limit\n",
      "{'loss': 3.3012, 'grad_norm': 8.55075454711914, 'learning_rate': 8.166666666666668e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3432, 'grad_norm': 10.521258354187012, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2555, 'grad_norm': 8.171425819396973, 'learning_rate': 7.833333333333333e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3996, 'grad_norm': 8.287679672241211, 'learning_rate': 7.666666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3939, 'grad_norm': 7.894793510437012, 'learning_rate': 7.5e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4039, 'grad_norm': 8.529488563537598, 'learning_rate': 7.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4713, 'grad_norm': 8.11582088470459, 'learning_rate': 7.166666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4828, 'grad_norm': 8.61213207244873, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3775, 'grad_norm': 8.792379379272461, 'learning_rate': 6.833333333333333e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4793, 'grad_norm': 8.728879928588867, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.09}\n",
      " 87%|█████████████████████████████████▊     | 2600/3000 [06:26<03:47,  1.76it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:15<00:15,  7.82s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:31<00:11, 11.07s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.059582, 'eval_rouge-2': 7.003784, 'eval_rouge-l': 23.993485999999997, 'eval_bleu-4': 0.032438559094625584, 'eval_runtime': 63.6324, 'eval_samples_per_second': 0.786, 'eval_steps_per_second': 0.063, 'epoch': 0.09}\n",
      " 87%|█████████████████████████████████▊     | 2600/3000 [07:30<03:47,  1.76it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:47<00:00, 12.80s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2600\n",
      "/root/miniconda3/envs/deepspeed/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /root/huggingface/hub/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Deleting older checkpoint [output/checkpoint-2100] due to args.save_total_limit\n",
      "{'loss': 3.3639, 'grad_norm': 8.151599884033203, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4248, 'grad_norm': 7.822229862213135, 'learning_rate': 6.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.5254, 'grad_norm': 7.739387512207031, 'learning_rate': 6.166666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4455, 'grad_norm': 8.607372283935547, 'learning_rate': 6e-06, 'epoch': 0.09}\n",
      "{'loss': 3.407, 'grad_norm': 8.23784065246582, 'learning_rate': 5.833333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3543, 'grad_norm': 8.087206840515137, 'learning_rate': 5.666666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4094, 'grad_norm': 8.6800537109375, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2738, 'grad_norm': 7.614261150360107, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4756, 'grad_norm': 8.71983814239502, 'learning_rate': 5.166666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4539, 'grad_norm': 9.001209259033203, 'learning_rate': 5e-06, 'epoch': 0.09}\n",
      " 90%|███████████████████████████████████    | 2700/3000 [08:29<03:03,  1.63it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.73s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:18<00:07,  7.46s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.12875, 'eval_rouge-2': 6.9813360000000015, 'eval_rouge-l': 24.173562, 'eval_bleu-4': 0.03450267756297807, 'eval_runtime': 37.409, 'eval_samples_per_second': 1.337, 'eval_steps_per_second': 0.107, 'epoch': 0.09}\n",
      " 90%|███████████████████████████████████    | 2700/3000 [09:06<03:03,  1.63it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:21<00:00,  5.34s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2700\n",
      "/root/miniconda3/envs/deepspeed/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /root/huggingface/hub/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Deleting older checkpoint [output/checkpoint-2200] due to args.save_total_limit\n",
      "{'loss': 3.4281, 'grad_norm': 8.365168571472168, 'learning_rate': 4.833333333333333e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2512, 'grad_norm': 7.728071212768555, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3783, 'grad_norm': 8.16408920288086, 'learning_rate': 4.5e-06, 'epoch': 0.1}\n",
      "{'loss': 3.392, 'grad_norm': 8.132630348205566, 'learning_rate': 4.333333333333334e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4592, 'grad_norm': 9.059975624084473, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.399, 'grad_norm': 8.168390274047852, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.1}\n",
      "{'loss': 3.352, 'grad_norm': 8.344497680664062, 'learning_rate': 3.833333333333334e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2654, 'grad_norm': 8.77767276763916, 'learning_rate': 3.666666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2799, 'grad_norm': 8.264272689819336, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2439, 'grad_norm': 7.839221954345703, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.1}\n",
      " 93%|████████████████████████████████████▍  | 2800/3000 [10:04<01:56,  1.71it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:15<00:15,  7.81s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:18<00:05,  5.60s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.12196200000001, 'eval_rouge-2': 7.351274, 'eval_rouge-l': 25.755092, 'eval_bleu-4': 0.03465555810166217, 'eval_runtime': 37.7114, 'eval_samples_per_second': 1.326, 'eval_steps_per_second': 0.106, 'epoch': 0.1}\n",
      " 93%|████████████████████████████████████▍  | 2800/3000 [10:42<01:56,  1.71it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:33<00:00,  9.27s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2800\n",
      "/root/miniconda3/envs/deepspeed/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /root/huggingface/hub/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Deleting older checkpoint [output/checkpoint-2300] due to args.save_total_limit\n",
      "{'loss': 3.4502, 'grad_norm': 7.83846378326416, 'learning_rate': 3.166666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3742, 'grad_norm': 8.144643783569336, 'learning_rate': 3e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3949, 'grad_norm': 8.180668830871582, 'learning_rate': 2.8333333333333335e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4445, 'grad_norm': 9.281543731689453, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4078, 'grad_norm': 8.676795959472656, 'learning_rate': 2.5e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3398, 'grad_norm': 8.587932586669922, 'learning_rate': 2.3333333333333336e-06, 'epoch': 0.1}\n",
      "{'loss': 3.377, 'grad_norm': 8.714742660522461, 'learning_rate': 2.166666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.5154, 'grad_norm': 9.285799980163574, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3059, 'grad_norm': 8.19896125793457, 'learning_rate': 1.8333333333333335e-06, 'epoch': 0.1}\n",
      "{'loss': 3.332, 'grad_norm': 9.112505912780762, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.1}\n",
      " 97%|█████████████████████████████████████▋ | 2900/3000 [11:41<00:56,  1.77it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.77s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.07s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.963719999999995, 'eval_rouge-2': 7.502933999999999, 'eval_rouge-l': 25.747734, 'eval_bleu-4': 0.03425591582902298, 'eval_runtime': 25.8965, 'eval_samples_per_second': 1.931, 'eval_steps_per_second': 0.154, 'epoch': 0.1}\n",
      " 97%|█████████████████████████████████████▋ | 2900/3000 [12:07<00:56,  1.77it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:21<00:00,  6.88s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2900\n",
      "/root/miniconda3/envs/deepspeed/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /root/huggingface/hub/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Deleting older checkpoint [output/checkpoint-2400] due to args.save_total_limit\n",
      "{'loss': 3.3062, 'grad_norm': 7.968064785003662, 'learning_rate': 1.5e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2471, 'grad_norm': 7.370285511016846, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3582, 'grad_norm': 8.842546463012695, 'learning_rate': 1.1666666666666668e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2563, 'grad_norm': 8.46739387512207, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 3.374, 'grad_norm': 8.102288246154785, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.1}\n",
      "{'loss': 3.21, 'grad_norm': 9.047545433044434, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.1}\n",
      "{'loss': 3.4512, 'grad_norm': 8.998976707458496, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.1}\n",
      "{'loss': 3.4295, 'grad_norm': 9.093742370605469, 'learning_rate': 3.3333333333333335e-07, 'epoch': 0.1}\n",
      "{'loss': 3.4764, 'grad_norm': 8.22502326965332, 'learning_rate': 1.6666666666666668e-07, 'epoch': 0.1}\n",
      "{'loss': 3.368, 'grad_norm': 7.89707612991333, 'learning_rate': 0.0, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [13:05<00:00,  1.81it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:15<00:15,  7.82s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:31<00:11, 11.22s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.746220000000005, 'eval_rouge-2': 6.789482000000001, 'eval_rouge-l': 24.234812, 'eval_bleu-4': 0.030655148911672852, 'eval_runtime': 51.6961, 'eval_samples_per_second': 0.967, 'eval_steps_per_second': 0.077, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [13:56<00:00,  1.81it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:47<00:00, 12.83s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-3000\n",
      "/root/miniconda3/envs/deepspeed/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /root/huggingface/hub/chatglm3-6b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "Deleting older checkpoint [output/checkpoint-2500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 836.9764, 'train_samples_per_second': 14.337, 'train_steps_per_second': 3.584, 'train_loss': 0.8998424479166667, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [13:56<00:00,  3.58it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      " 51%|█████████████████████▊                     | 34/67 [06:44<05:32, 10.06s/it]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python finetune_demo/finetune_hf.py  data/AdvertiseGen_fix  /root/huggingface/hub/chatglm3-6b  finetune_demo/configs/lora.yaml yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train/loss\n",
    "\n",
    "![train/loss](zhipu.train_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 你需要自己适配 inference.py 中的代码，并迁移到其他的推理框架中。例如，basic_demo 中没有读取微调模型后的 adapter 的内容，你需要参考 inference.py 的代码并进行修改，让其他 demo 能读入你的微调代码，将其部署到 basic_demo 下的 gradio_demo 中，并能够通过 webui 来进行调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选定loss较低的 output/checkoutpoint-2800 ，更新 basic_demo/web_demo_gradio.py  \n",
    "\n",
    "```python\n",
    "MODEL_PATH = os.environ.get('MODEL_PATH', '/root/projects/LLM-quickstart/homework/output/checkpoint-2800')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:03<00:00,  1.80it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "\n",
      "\n",
      "====conversation====\n",
      " [{'role': 'user', 'content': '类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则'}]\n",
      "\n",
      "\n",
      "====conversation====\n",
      " [{'role': 'user', 'content': '类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则'}, {'role': 'assistant', 'content': '一件时尚的纯色系连衣裙，采用小碎花图案进行点缀。别致的木耳边装饰在领口和袖子处，搭配上透视网纱元素的设计，使得整体更加优雅浪漫，也更具女人味儿；修身的剪裁设计修身又显高挑，而肩部及腰部的开叉处理更是让这款连衣裙充满了性感的气息！同时，加上侧边的隐形口袋以及前幅的金属扣锁带来方便实用性的同时在视觉上更具有立体感。'}, {'role': 'user', 'content': '类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则'}]\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "! python basic_demo/web_demo_gradio.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### infer_gradio\n",
    "![infer_gradio](zhipu.infer_gradio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
